import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
import os
from torch.optim.lr_scheduler import StepLR

class AMTENNet(nn.Module):
    def __init__(self):
        super(AMTENNet, self).__init__()

        # Conv1: Predict pixel values (3 in -> 3 out)
        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)

        # Composite Conv2 + Conv3: F1
        self.conv2 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(3)
        self.conv3 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(3)

        # Composite Conv4 + Conv5: F2
        self.conv4 = nn.Conv2d(6, 6, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(6)
        self.conv5 = nn.Conv2d(6, 6, kernel_size=3, stride=1, padding=1)
        self.bn5 = nn.BatchNorm2d(6)

        # HFE: Conv6 -> Conv7 -> Conv8 -> Conv9
        self.conv6 = nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1)
        self.bn6 = nn.BatchNorm2d(24)

        self.conv7 = nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1)
        self.bn7 = nn.BatchNorm2d(48)

        self.conv8 = nn.Conv2d(48, 64, kernel_size=3, stride=1, padding=1)
        self.bn8 = nn.BatchNorm2d(64)

        self.conv9 = nn.Conv2d(64, 128, kernel_size=1, stride=1)  # No BN here

        # Classification Head
        self.fc1 = nn.Linear(128 * 7 * 7, 300)
        self.fc2 = nn.Linear(300, 300)
        self.fc3 = nn.Linear(300, 2)  # Output logits for 2 classes

        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)

    def forward(self, x,return_features=False):
        pred = self.conv1(x)
        fmt = pred - x  # Residual (manipulation traces)
       
        f1_inter = F.relu(self.bn2(self.conv2(fmt)))
        f1 = F.relu(self.bn3(self.conv3(f1_inter)))
        f1_concat = torch.cat([f1, fmt], dim=1)

        f2_inter = F.relu(self.bn4(self.conv4(f1_concat)))
        f2 = F.relu(self.bn5(self.conv5(f2_inter)))
        freu = torch.cat([f2, f1_concat], dim=1)

        # HFE (match Caffe prototxt order: Conv -> Pool -> ReLU -> BN)
        out = self.conv6(freu)
        out = self.pool(out)
        out = F.relu(out)
        out = self.bn6(out)

        out = self.conv7(out)
        out = self.pool(out)
        out = F.relu(out)
        out = self.bn7(out)

        out = self.conv8(out)
        out = self.pool(out)
        out = F.relu(out)
        out = self.bn8(out)

        out = self.conv9(out)
        out = self.pool(out)
        out = F.relu(out)
        # Conv9 has no BN

        out = out.view(out.size(0), -1)
        out = F.relu(self.fc1(out))
        out = F.relu(self.fc2(out))
        logits = self.fc3(out)

        if return_features:
            return logits, fmt, f1, f2, freu
        else:
            return logits

# Data loading and preprocessing
def load_datasets(base_path):
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5,0.5])  # Normalize to [-1, 1]
    ])

    train_dir = os.path.join(base_path, 'train')
    val_dir = os.path.join(base_path, 'valid')

    train_dataset = datasets.ImageFolder(train_dir, transform=transform)
    val_dataset = datasets.ImageFolder(val_dir, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)

    return train_loader, val_loader

# Training loop function
def train_amtennet(model, train_loader, val_loader, device):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.95, weight_decay=0.005)
    scheduler = StepLR(optimizer, step_size=20, gamma=0.5)  # Halve LR every 5 epochs

    model.to(device)
    best_val_acc = 0.0
    best_model_path = ""

    for epoch in range(100):  # 10 epochs
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/100 [Train]", leave=False)
        for images, labels in loop:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            loop.set_postfix(loss=loss.item(), acc=100 * correct / total)

        scheduler.step()  # Step LR scheduler once per epoch

        train_acc = 100 * correct / total
        print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_acc:.2f}%")

        # Validation phase
        model.eval()
        correct_val = 0
        total_val = 0
        val_loop = tqdm(val_loader, desc=f"Epoch {epoch+1}/100 [Val]", leave=False)
        with torch.no_grad():
            for images, labels in val_loop:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                preds = outputs.argmax(dim=1)
                correct_val += (preds == labels).sum().item()
                total_val += labels.size(0)

        val_acc = 100 * correct_val / total_val
        print(f"Validation Accuracy = {val_acc:.2f}%\n")

        # Save checkpoint
        checkpoint_path = f"amtennet_epoch.pt"
        torch.save(model.state_dict(), checkpoint_path)
        print(f"Checkpoint saved: {checkpoint_path}\n")

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_path = f"amtennet_best.pt"
            torch.save(model.state_dict(), best_model_path)
            print(f"Best model updated: {best_model_path} (Val Acc:{best_val_acc:.2f}%)\n")

    print(f"\nFinal Best Model: {best_model_path} with Accuracy: {best_val_acc:.2f}%")

# Main script to run training
def main():
    base_path = "/home/user/Deepfake/Datasets/real_vs_fake/real-vs-fake"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_loader, val_loader = load_datasets(base_path)
    model = AMTENNet()
    train_amtennet(model, train_loader, val_loader, device)

if __name__ == '__main__':
    main()
